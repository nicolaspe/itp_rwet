{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# morning drinks for existencial questions\n",
    "\n",
    "by nicolÃ¡s escarpentier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of this notebook\n",
    "I want to be able to switch the source dialogs, but the recipe structure will always be the same. At the same time, the list of target questions and answers will be something I input. \n",
    "\n",
    "The flow of this notebook will be the following:\n",
    "- load recipe structures\n",
    "- create tracery grammar rules\n",
    "- create dialog loader\n",
    "  - this creates the lists of tokens and rest of the stuff\n",
    "- construct functions that let me use all from above with the input of q&a targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages\n",
    "import random as rng\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracery\n",
    "import tracery\n",
    "from tracery.modifiers import base_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe scraper package\n",
    "from recipe_scrapers import scrape_me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions\n",
    "I will also define some base functions that have been used in class and are not specific to this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list functions\n",
    "def remove_all(bye, words):\n",
    "    while(bye in words):\n",
    "        words.remove(bye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector addition\n",
    "def addv(coord1, coord2):\n",
    "    return [c1 + c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "# vector subtraction\n",
    "def subtractv(coord1, coord2):\n",
    "    return [c1 - c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "# vector average\n",
    "def meanv(coords):\n",
    "    # assumes every item in coords has same length as item 0\n",
    "    sumv = [0] * len(coords[0])\n",
    "    for item in coords:\n",
    "        for i in range(len(item)):\n",
    "            sumv[i] += item[i]\n",
    "    mean = [0] * len(sumv)\n",
    "    for i in range(len(sumv)):\n",
    "        mean[i] = float(sumv[i]) / len(coords)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spacy vector\n",
    "def vec(s):\n",
    "    return nlp.vocab[s].vector\n",
    "\n",
    "# get spacy sentence vector\n",
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return meanv([w.vector for w in sent])\n",
    "\n",
    "# cosine similarity\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# closest word to target vector from token list\n",
    "def spacy_closest(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list, key=lambda x: cosine(vec_to_check, vec(x)), reverse=True)[:n]\n",
    "\n",
    "# closest sentence to target vector from token list\n",
    "def spacy_closest_sent(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list, key=lambda x: cosine(vec_to_check, sentvec(x)), reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recipe structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### units and extra ingredients\n",
    "\n",
    "First, the extra ingredients and ingredients unit list is just made by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingr_units = [\"fluid oz.\",\n",
    "             \"tablespoons\",\n",
    "             \"teaspoons\",\n",
    "             \"oz.\"]\n",
    "\n",
    "ingr_extra = [\"fresh mint leaves\",\n",
    "             \"1 lime, cut into wedges\",\n",
    "             \"ice cubes\",\n",
    "             \"rimming salt\",\n",
    "             \"1 orange, sliced\",\n",
    "             \"twist lime zest\",\n",
    "             \"maraschino cherries\",\n",
    "             \"pineapple wedges\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instructions\n",
    "\n",
    "Now, onto creating the scraped instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_tracer(s):\n",
    "    # look for the verb on the sentence\n",
    "    verb = \"\"\n",
    "    for word in s:\n",
    "        if word.tag_ == \"VB\":\n",
    "            verb = word\n",
    "    # if there's no verb, return \"\"\n",
    "    if verb is \"\":\n",
    "        return \"\"\n",
    "    # if the verb has children, go through them looking for \"prep\" and \"dobj\"\n",
    "    elif len(list(verb.children)) > 0:\n",
    "        # get the prep\n",
    "        prep_children = [ch for ch in list(verb.children) if ch.dep_ == \"prep\"]\n",
    "        # join them for tracery\n",
    "        prep_text = \" \".join([ch.text+\" #np#\" for ch in prep_children])\n",
    "        # get the dobj\n",
    "        dobj_children = [ch for ch in list(verb.children) if ch.dep_ == \"dobj\"]\n",
    "        # joint them as a string\n",
    "        dobj_text = \"\"\n",
    "        for ch in dobj_children:\n",
    "            dobj_text += \" \".join([word.text for word in ch.subtree])\n",
    "        # get the noun_chunks from the sentence and replace them with\n",
    "        # a tracery placeholder in the dobj_text\n",
    "        chunks = s.noun_chunks\n",
    "        for ch in chunks:\n",
    "            dobj_text = dobj_text.replace(ch.text, \"#np#\")\n",
    "        # return the beautiful phrase\n",
    "        return verb.text + \" \" + dobj_text + \" \" + prep_text\n",
    "    # else, just return the verb + a tracery placeholder\n",
    "    else:\n",
    "        return verb.text+\" #np#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the full recipes\n",
    "drinks_sources = [line.strip() for line in open('./recipe_sources.txt').readlines()]\n",
    "drinks_scraped = [scrape_me(item) for item in drinks_sources]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the instructions from the recipes\n",
    "drinks_instructions = []\n",
    "[drinks_instructions.append(drink.instructions()) for drink in drinks_scraped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nlp to separate the sentences and have all the data we need\n",
    "nlp_instructions = [list(nlp(inst).sents) for inst in drinks_instructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the finishing touches\n",
    "instr_finish = [instr[-1].text.strip() for instr in nlp_instructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate the instruction bodies\n",
    "instr_nlp_body = []\n",
    "[instr_nlp_body.extend(instr[:-1]) for instr in nlp_instructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and create the instructions ready for tracery\n",
    "instr_body = [verb_tracer(instr).strip() for instr in instr_nlp_body]\n",
    "remove_all('', instr_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tracery grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dialog loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn everything into a function!\n",
    "def dialog_loader(file_name):\n",
    "    # load the dialog and use nlp to get the sentences\n",
    "    dialog = [line.strip() for line in open(file_name).readlines()]\n",
    "    dialog_sents = [line.text.strip() for line in list( nlp(' '.join(dialog)).sents )]\n",
    "    # get the questions and answers\n",
    "    q_a_lines = [[q_line, a_line] for q_line,a_line in zip(dialog_sents, dialog_sents[1:]) if '?' in q_line]\n",
    "    # return these pairs!\n",
    "    return q_a_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final functions\n",
    "\n",
    "This is what puts everything together and are the only places where you'd have to enter inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drink(q_target, a_target):\n",
    "    # == DEFINE FROM DIALOG SOURCE\n",
    "    # define a question according to the target\n",
    "    all_questions = spacy_closest_sent( dialog_q, vec(q_target), 5 )\n",
    "    question = rng.choice( all_questions )\n",
    "    print(question)\n",
    "    print('')\n",
    "    # get the target noun_chunk to \"solve\" and create the solution vector (question to target) \n",
    "    question_chunks = [ch.text for ch in nlp(question).noun_chunks]\n",
    "    selected_chunk = rng.choice(question_chunks)\n",
    "    solution_vector = subtractv(vec(q_target), sentvec(selected_chunk))\n",
    "    # get the answer noun_chunks > the ingredients\n",
    "    a_vector = vec(a_target)\n",
    "    all_ingredients = spacy_closest(ingr_tokens, addv(solution_vector, a_vector), 15)\n",
    "    # == INGREDIENTS\n",
    "    # make an ingredients list\n",
    "    if len(all_ingredients) >= 6:\n",
    "        ingredients = rng.sample(all_ingredients, rng.randrange(3,6))\n",
    "    else:\n",
    "        ingredients = all_ingredients\n",
    "    # and write the list with the amounts\n",
    "    for ingr in ingredients:\n",
    "        amnt = rng.randrange(1, 4)\n",
    "        unit = rng.choice(ingr_units)\n",
    "        print( str(amnt) + ' ' + unit + ' of ' + ingr )\n",
    "    # add random extra ingredient\n",
    "    print( rng.choice(ingr_extra) )\n",
    "    print('')\n",
    "    # == INSTRUCTIONS\n",
    "    # select the instructions\n",
    "    instructions = ''\n",
    "    for i in range(rng.randrange(3, 5)):\n",
    "        # select a random instruction\n",
    "        instructions += rng.choice(instr_body) + '\\n'\n",
    "    # replace the placeholders with ingredients in the order of the list, \n",
    "    # overflowing if the index goes out of bounds\n",
    "    ingr_index = 0\n",
    "    while \"#np#\" in instructions:\n",
    "        instructions = instructions.replace(\"#np#\", ingredients[ingr_index], 1)\n",
    "        ingr_index = (ingr_index+1)%len(ingredients)\n",
    "    # add the finishing instruction\n",
    "    instructions += rng.choice( instr_finish )\n",
    "    # and print the instructions\n",
    "    print(instructions)\n",
    "    print(\"\\n==========\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename input\n",
    "filename = \"./hp7_dialog.txt\"\n",
    "# load dialog and create the lists for q&a + answer noun_chunks > the ingredients (global)\n",
    "dialog = dialog_loader(filename)\n",
    "dialog_q = [pair[0] for pair in dialog]\n",
    "dialog_a = [pair[1] for pair in dialog]\n",
    "ingr_tokens = list(set( [ch.text for ch in nlp( ' '.join(dialog_a) ).noun_chunks] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXECUTE !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Didn't I tell you this nutter was just raving as usual?\n",
      "\n",
      "2 fluid oz. of pain\n",
      "2 teaspoons of life\n",
      "3 fluid oz. of soul\n",
      "3 teaspoons of sense\n",
      "1 oz. of longing\n",
      "pineapple wedges\n",
      "\n",
      "add pain and life\n",
      "Dip soul of sense in longing into pain\n",
      "combine\n",
      "serve  over life\n",
      "Add ice and stir.\n",
      "\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the targets: what to solve in the question and what you want to achieve in the answer\n",
    "question_target = \"tired\"\n",
    "answer_target = \"happiness\"\n",
    "# and execute!\n",
    "get_drink(question_target, answer_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now create :D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mind, who's to say they haven't already caught and killed him without publicizing it?\"\n",
      "\n",
      "2 fluid oz. of centuries\n",
      "3 tablespoons of lips\n",
      "2 oz. of Nerves\n",
      "pineapple wedges\n",
      "\n",
      "punch centuries\n",
      "punch lips\n",
      "fill Nerves with centuries\n",
      "Garnish with a lime wedge.\n",
      "\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2, hp7, from stressed to rested\n",
    "question_target = \"stressed\"\n",
    "answer_target = \"rested\"\n",
    "get_drink(question_target, answer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you feeling?\n",
      "\n",
      "2 teaspoons of desperation\n",
      "3 tablespoons of panic\n",
      "2 tablespoons of excitement\n",
      "ice cubes\n",
      "\n",
      "Pour desperation of panic onto excitement\n",
      "Mix desperation of your favorite sparkling white to panic\n",
      "Pour  in excitement\n",
      "Fill desperation\n",
      "Garnish with maraschino cherry.\n",
      "\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3, hp7, from anxious to ecstatic\n",
    "question_target = \"anxious\"\n",
    "answer_target = \"ecstatic\"\n",
    "get_drink(question_target, answer_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making a more dialog function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drink2(q_target, a_target):\n",
    "    # == DEFINE FROM DIALOG SOURCE\n",
    "    # define a question according to the target\n",
    "    all_questions = spacy_closest_sent( dialog_q, vec(q_target), 5 )\n",
    "    question = rng.choice( all_questions )\n",
    "    all_questions.remove(question)\n",
    "    print(question)\n",
    "    for q in all_questions:\n",
    "        print(q)\n",
    "    print('')\n",
    "    # get the target noun_chunk to \"solve\" and create the solution vector (question to target) \n",
    "    question_chunks = [ch.text for ch in nlp(question).noun_chunks]\n",
    "    selected_chunk = rng.choice(question_chunks)\n",
    "    solution_vector = subtractv(vec(q_target), sentvec(selected_chunk))\n",
    "    # get the answer noun_chunks > the ingredients\n",
    "    a_vector = vec(a_target)\n",
    "    all_ingredients = spacy_closest(ingr_tokens, addv(solution_vector, a_vector), 15)\n",
    "    # == INGREDIENTS\n",
    "    # make an ingredients list\n",
    "    if len(all_ingredients) >= 6:\n",
    "        ingredients = rng.sample(all_ingredients, rng.randrange(3,6))\n",
    "    else:\n",
    "        ingredients = all_ingredients\n",
    "    # and write the list with the amounts\n",
    "    for ingr in ingredients:\n",
    "        amnt = rng.randrange(1, 4)\n",
    "        unit = rng.choice(ingr_units)\n",
    "        print( str(amnt) + ' ' + unit + ' of ' + ingr )\n",
    "    # add random extra ingredient\n",
    "    print( rng.choice(ingr_extra) )\n",
    "    print('')\n",
    "    # == INSTRUCTIONS\n",
    "    # select the instructions\n",
    "    instructions = ''\n",
    "    for i in range(rng.randrange(3, 5)):\n",
    "        # select a random instruction\n",
    "        instructions += rng.choice(instr_body) + '\\n'\n",
    "    # replace the placeholders with ingredients in the order of the list, \n",
    "    # overflowing if the index goes out of bounds\n",
    "    ingr_index = 0\n",
    "    while \"#np#\" in instructions:\n",
    "        instructions = instructions.replace(\"#np#\", ingredients[ingr_index], 1)\n",
    "        ingr_index = (ingr_index+1)%len(ingredients)\n",
    "    # add the finishing instruction\n",
    "    instructions += rng.choice( instr_finish )\n",
    "    # and print the instructions\n",
    "    print(instructions)\n",
    "    print(\"\\n==========\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It wasn't all bad, was it?\n",
      "Who had it, Ducain?\n",
      "Wasn't he a war hero?!\n",
      "What happened -- did he hurt you?\n",
      "I've had a pretty messed up day, alright?!\n",
      "\n",
      "1 fluid oz. of They\n",
      "2 tablespoons of him\n",
      "3 tablespoons of I\n",
      "1 teaspoons of them\n",
      "pineapple wedges\n",
      "\n",
      "release They oils and him juice\n",
      "chill\n",
      "Dip I of them in They into him\n",
      "Makes 9 delicious drinks, perfect for any ladies night in.\n",
      "\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4, sw7, from lost to decided\n",
    "\n",
    "# filename input\n",
    "filename = \"./sw7_dialog.txt\"\n",
    "# load dialog and create the lists for q&a + answer noun_chunks > the ingredients (global)\n",
    "dialog = dialog_loader(filename)\n",
    "dialog_q = [pair[0] for pair in dialog]\n",
    "dialog_a = [pair[1] for pair in dialog]\n",
    "ingr_tokens = list(set( [ch.text for ch in nlp( ' '.join(dialog_a) ).noun_chunks] ))\n",
    "\n",
    "question_target = \"lost\"\n",
    "answer_target = \"decided\"\n",
    "get_drink2(question_target, answer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5, hp5, from lost to decided\n",
    "\n",
    "# filename input\n",
    "filename = \"./hp5_dialog.txt\"\n",
    "# load dialog and create the lists for q&a + answer noun_chunks > the ingredients (global)\n",
    "dialog = dialog_loader(filename)\n",
    "dialog_q = [pair[0] for pair in dialog]\n",
    "dialog_a = [pair[1] for pair in dialog]\n",
    "ingr_tokens = list(set( [ch.text for ch in nlp( ' '.join(dialog_a) ).noun_chunks] ))\n",
    "\n",
    "question_target = \"lost\"\n",
    "answer_target = \"decided\"\n",
    "get_drink2(question_target, answer_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
