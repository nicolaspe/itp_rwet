{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# morning drinks for existencial questions\n",
    "\n",
    "by nicolÃ¡s escarpentier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of this notebook\n",
    "I want to be able to switch the source dialogs, but the recipe structure will always be the same. At the same time, the list of target questions and answers will be something I input. \n",
    "\n",
    "The flow of this notebook will be the following:\n",
    "- load recipe structures\n",
    "- create tracery grammar rules\n",
    "- create dialog loader\n",
    "  - this creates the lists of tokens and rest of the stuff\n",
    "- construct functions that let me use all from above with the input of q&a targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages\n",
    "import random as rng\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracery\n",
    "import tracery\n",
    "from tracery.modifiers import base_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe scraper package\n",
    "from recipe_scrapers import scrape_me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions\n",
    "I will also define some base functions that have been used in class and are not specific to this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list functions\n",
    "def remove_all(bye, words):\n",
    "    while(bye in words):\n",
    "        words.remove(bye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector addition\n",
    "def addv(coord1, coord2):\n",
    "    return [c1 + c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "# vector subtraction\n",
    "def subtractv(coord1, coord2):\n",
    "    return [c1 - c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "# vector average\n",
    "def meanv(coords):\n",
    "    # assumes every item in coords has same length as item 0\n",
    "    sumv = [0] * len(coords[0])\n",
    "    for item in coords:\n",
    "        for i in range(len(item)):\n",
    "            sumv[i] += item[i]\n",
    "    mean = [0] * len(sumv)\n",
    "    for i in range(len(sumv)):\n",
    "        mean[i] = float(sumv[i]) / len(coords)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spacy vector\n",
    "def vec(s):\n",
    "    return nlp.vocab[s].vector\n",
    "\n",
    "# get spacy sentence vector\n",
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return meanv([w.vector for w in sent])\n",
    "\n",
    "# cosine similarity\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# closest word to target vector from token list\n",
    "def spacy_closest(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list, key=lambda x: cosine(vec_to_check, vec(x)), reverse=True)[:n]\n",
    "\n",
    "# closest sentence to target vector from token list\n",
    "def spacy_closest_sent(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list, key=lambda x: cosine(vec_to_check, sentvec(x)), reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recipe structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### units and extra ingredients\n",
    "\n",
    "First, the extra ingredients and ingredients unit list is just made by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingr_units = [\"fluid oz.\",\n",
    "             \"tablespoons\",\n",
    "             \"teaspoons\",\n",
    "             \"oz.\"]\n",
    "\n",
    "ingr_extra = [\"fresh mint leaves\",\n",
    "             \"1 lime, cut into wedges\",\n",
    "             \"ice cubes\",\n",
    "             \"rimming salt\",\n",
    "             \"1 orange, sliced\",\n",
    "             \"twist lime zest\",\n",
    "             \"maraschino cherries\",\n",
    "             \"pineapple wedges\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instructions\n",
    "\n",
    "Now, onto creating the scraped instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_tracer(s):\n",
    "    # look for the verb on the sentence\n",
    "    verb = \"\"\n",
    "    for word in s:\n",
    "        if word.tag_ == \"VB\":\n",
    "            verb = word\n",
    "    # if there's no verb, return \"\"\n",
    "    if verb is \"\":\n",
    "        return \"\"\n",
    "    # if the verb has children, go through them looking for \"prep\" and \"dobj\"\n",
    "    elif len(list(verb.children)) > 0:\n",
    "        # get the prep\n",
    "        prep_children = [ch for ch in list(verb.children) if ch.dep_ == \"prep\"]\n",
    "        # join them for tracery\n",
    "        prep_text = \" \".join([ch.text+\" #np#\" for ch in prep_children])\n",
    "        # get the dobj\n",
    "        dobj_children = [ch for ch in list(verb.children) if ch.dep_ == \"dobj\"]\n",
    "        # joint them as a string\n",
    "        dobj_text = \"\"\n",
    "        for ch in dobj_children:\n",
    "            dobj_text += \" \".join([word.text for word in ch.subtree])\n",
    "        # get the noun_chunks from the sentence and replace them with\n",
    "        # a tracery placeholder in the dobj_text\n",
    "        chunks = s.noun_chunks\n",
    "        for ch in chunks:\n",
    "            dobj_text = dobj_text.replace(ch.text, \"#np#\")\n",
    "        # return the beautiful phrase\n",
    "        return verb.text + \" \" + dobj_text + \" \" + prep_text\n",
    "    # else, just return the verb + a tracery placeholder\n",
    "    else:\n",
    "        return verb.text+\" #np#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the full recipes\n",
    "drinks_sources = [line.strip() for line in open('./recipe_sources.txt').readlines()]\n",
    "drinks_scraped = [scrape_me(item) for item in drinks_sources]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the instructions from the recipes\n",
    "drinks_instructions = []\n",
    "[drinks_instructions.append(drink.instructions()) for drink in drinks_scraped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nlp to separate the sentences and have all the data we need\n",
    "nlp_instructions = [list(nlp(inst).sents) for inst in drinks_instructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the finishing touches\n",
    "instr_finish = [instr[-1].text.strip() for instr in nlp_instructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate the instruction bodies\n",
    "instr_nlp_body = []\n",
    "[instr_nlp_body.extend(instr[:-1]) for instr in nlp_instructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and create the instructions ready for tracery\n",
    "instr_body = [verb_tracer(instr).strip() for instr in instr_nlp_body]\n",
    "remove_all('', instr_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dialog loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn everything into a function!\n",
    "def dialog_loader(file_name):\n",
    "    # load the dialog and use nlp to get the sentences\n",
    "    dialog = [line.strip() for line in open(file_name).readlines()]\n",
    "    dialog_sents = [line.text.strip() for line in list( nlp(' '.join(dialog)).sents )]\n",
    "    # get the questions and answers\n",
    "    q_a_lines = [[q_line, a_line] for q_line,a_line in zip(dialog_sents, dialog_sents[1:]) if '?' in q_line]\n",
    "    # return these pairs!\n",
    "    return q_a_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final functions\n",
    "\n",
    "This is what puts everything together and are the only places where you'd have to enter inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename input\n",
    "filename = \"./sw7_dialog.txt\"\n",
    "# load dialog and create the lists for q&a + answer noun_chunks > the ingredients (global)\n",
    "dialog = dialog_loader(filename)\n",
    "dialog_q = [pair[0] for pair in dialog]\n",
    "dialog_a = [pair[1] for pair in dialog]\n",
    "verbs_q  =    list(set( [vb.text.lower() for vb in nlp( ' '.join(dialog_q) ) if vb.tag_ == \"VB\"] ))\n",
    "verbs_a  =    list(set( [vb.text.lower() for vb in nlp( ' '.join(dialog_a) ) if vb.tag_ == \"VB\"] ))\n",
    "ingr_tokens = list(set( [ch.text.lower() for ch in nlp( ' '.join(dialog_a) ).noun_chunks] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXECUTE !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_develop = [\"Do you feel like #qvb#ing yourself?\",\n",
    "             \"Obsessed with #qvb#ing.\",\n",
    "             \"Oppressed by #qvb#ing.\",\n",
    "             \"Does it get better?\",\n",
    "             \"Will it get better?\",\n",
    "             \"Get up and #qvb#ing.\",\n",
    "             \"Sleep or stay awake.\",\n",
    "             \"Waves of me #qvb#ing feelings\",\n",
    "             \"Sometimes the times are pressing, the constraints on top of your head\",\n",
    "             \"The days and weeks are neverending.\"]\n",
    "\n",
    "a_develop = [\" for 10 seconds.\",\n",
    "             \" to taste.\",\n",
    "             \". Consider your actions during the day.\",\n",
    "             \". What you did yesterday was fine.\",\n",
    "             \" until your confidence resurfaces.\",\n",
    "             \". Strive for your dreams.\",\n",
    "             \" and hear your feelings.\",\n",
    "             \" and sit down for 5 second, your body needs rest.\",\n",
    "             \". You are a great person.\",\n",
    "             \" and fuck the opinion of critics.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def develop_questions(q):\n",
    "    first_bit = q +\" \"\n",
    "    verb_closests = spacy_closest(q_develop, vec(q), 10)\n",
    "    for i in range(rng.randrange(5,9)):\n",
    "        first_bit += rng.choice(verb_closests)+\" \"\n",
    "        first_bit = first_bit.replace(\"#qvb#\", rng.choice(verbs_q))\n",
    "    print(first_bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def develop_answers(ingred):\n",
    "    aa = ''\n",
    "    for ingr in ingred:\n",
    "        amnt = rng.randrange(1, 4)\n",
    "        unit = rng.choice(ingr_units)\n",
    "        aa += str(amnt) + ' ' + unit + ' of ' + ingr\n",
    "        aa += rng.choice(a_develop) + '\\n'\n",
    "    print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def develop_instructions(ingred):\n",
    "    # select the instructions\n",
    "    instructions = ''\n",
    "    for i in range(rng.randrange(3, 5)):\n",
    "        # select a random instruction\n",
    "        instructions += rng.choice(instr_body) + rng.choice(a_develop) + '\\n'\n",
    "    # replace the placeholders with ingredients in the order of the list, \n",
    "    # overflowing if the index goes out of bounds\n",
    "    ingr_index = 0\n",
    "    while \"#np#\" in instructions:\n",
    "        instructions = instructions.replace(\"#np#\", ingred[ingr_index], 1)\n",
    "        ingr_index = (ingr_index+1)%len(ingred)\n",
    "    # add the finishing instruction\n",
    "    instructions += rng.choice( instr_finish )\n",
    "    print(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drink_new(q_target, a_target):\n",
    "    # == DEFINE FROM DIALOG SOURCE\n",
    "    # define a question according to the target\n",
    "    all_questions = spacy_closest_sent( dialog_q, vec(q_target), 5 )\n",
    "    question = rng.choice( all_questions )\n",
    "    develop_questions(question)\n",
    "    # get the target noun_chunk to \"solve\" and create the solution vector (question to target) \n",
    "    question_chunks = [ch.text for ch in nlp(question).noun_chunks]\n",
    "    selected_chunk = rng.choice(question_chunks)\n",
    "    solution_vector = subtractv(vec(q_target), sentvec(selected_chunk))\n",
    "    # get the answer noun_chunks > the ingredients\n",
    "    a_vector = vec(a_target)\n",
    "    all_ingredients = spacy_closest(ingr_tokens, addv(solution_vector, a_vector), 15)\n",
    "    # == INGREDIENTS\n",
    "    # get closest 2 ingredients + 2 random\n",
    "    ingredients = all_ingredients[0:2]\n",
    "    ingredients.extend(rng.sample(all_ingredients,2))\n",
    "    # and write the list with the amounts\n",
    "    print()\n",
    "    for ingr in ingredients:\n",
    "        amnt = rng.randrange(1, 4)\n",
    "        unit = rng.choice(ingr_units)\n",
    "        print( str(amnt) + ' ' + unit + ' of ' + ingr )\n",
    "    # add random extra ingredient\n",
    "    print( rng.choice(ingr_extra) )\n",
    "    print('')\n",
    "    # == INSTRUCTIONS\n",
    "    # select the instructions\n",
    "    develop_instructions(ingredients)\n",
    "    print(\"\\n==========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have you felt it? Oppressed by leaveing. The days and weeks are neverending. Get up and trying. Sleep or stay awake. Oppressed by hearing. \n",
      "\n",
      "3 teaspoons of transportation\n",
      "3 teaspoons of sanitation\n",
      "2 oz. of rumors\n",
      "2 fluid oz. of us\n",
      "pineapple wedges\n",
      "\n",
      "shake transportation until your confidence resurfaces.\n",
      "Fill sanitation with rumors and fuck the opinion of critics.\n",
      "pour  into us and fuck the opinion of critics.\n",
      "release transportation. Strive for your dreams.\n",
      "Fill the glasses with club soda, stir, and garnish with additional lime wedges.\n",
      "\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_target = \"anxiety\"\n",
    "answer_target = \"vacations\"\n",
    "get_drink_new(question_target, answer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you ever hear the tragedy of Darth Plagueis \"the wise\"? Obsessed with accomplishing. The days and weeks are neverending. The days and weeks are neverending. Do you feel like destroying yourself? Waves of me takeing feelings \n",
      "\n",
      "1 fluid oz. of she\n",
      "1 fluid oz. of me\n",
      "2 fluid oz. of nothing\n",
      "2 teaspoons of everything\n",
      "1 orange, sliced\n",
      "\n",
      "serve  over she. You are a great person.\n",
      "Pour  in me and sit down for 5 second, your body needs rest.\n",
      "Pour nothing , everything and she into me and fuck the opinion of critics.\n",
      "Garnish with a lime twist.\n",
      "\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filename input\n",
    "filename = \"./sw3_dialog.txt\"\n",
    "# load dialog and create the lists for q&a + answer noun_chunks > the ingredients (global)\n",
    "dialog = dialog_loader(filename)\n",
    "dialog_q = [pair[0] for pair in dialog]\n",
    "dialog_a = [pair[1] for pair in dialog]\n",
    "verbs_q  =    list(set( [vb.text.lower() for vb in nlp( ' '.join(dialog_q) ) if vb.tag_ == \"VB\"] ))\n",
    "verbs_a  =    list(set( [vb.text.lower() for vb in nlp( ' '.join(dialog_a) ) if vb.tag_ == \"VB\"] ))\n",
    "ingr_tokens = list(set( [ch.text.lower() for ch in nlp( ' '.join(dialog_a) ).noun_chunks] ))\n",
    "\n",
    "question_target = \"melancholy\"\n",
    "answer_target = \"summer\"\n",
    "get_drink_new(question_target, answer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"What's worse than death?\" Sometimes the times are pressing, the constraints on top of your head The days and weeks are neverending. Oppressed by disarming. Waves of me understanding feelings Get up and hateing. Sleep or stay awake. \n",
      "\n",
      "2 teaspoons of memories\n",
      "3 tablespoons of death\n",
      "1 tablespoons of feelings\n",
      "3 fluid oz. of glee\n",
      "fresh mint leaves\n",
      "\n",
      "Rub memories around death. You are a great person.\n",
      "strain  into feelings. Strive for your dreams.\n",
      "shake glee. What you did yesterday was fine.\n",
      "Fill memories for 10 seconds.\n",
      "Pour into glasses and serve.\n",
      "\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filename input\n",
    "filename = \"./hp5_dialog.txt\"\n",
    "# load dialog and create the lists for q&a + answer noun_chunks > the ingredients (global)\n",
    "dialog = dialog_loader(filename)\n",
    "dialog_q = [pair[0] for pair in dialog]\n",
    "dialog_a = [pair[1] for pair in dialog]\n",
    "verbs_q  =    list(set( [vb.text.lower() for vb in nlp( ' '.join(dialog_q) ) if vb.tag_ == \"VB\"] ))\n",
    "verbs_a  =    list(set( [vb.text.lower() for vb in nlp( ' '.join(dialog_a) ) if vb.tag_ == \"VB\"] ))\n",
    "ingr_tokens = list(set( [ch.text.lower() for ch in nlp( ' '.join(dialog_a) ).noun_chunks] ))\n",
    "\n",
    "question_target = \"death\"\n",
    "answer_target = \"memories\"\n",
    "get_drink_new(question_target, answer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
